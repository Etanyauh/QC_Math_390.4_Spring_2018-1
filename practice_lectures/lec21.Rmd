---
title: "Lecture 21 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 30, 2018"
---

# Logistic Regression for Binary Response

Let's load up the cancer dataset, remove missing data, remove the ID column and add good names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've done this awhile ago!!!
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's code "malignant" as 1 and "benign" as 0.

```{r}
biopsy$class = ifelse(biopsy$class == "malignant", 1, 0)
```


Now let's split into training and test for experiments:

```{r}
test_prop = 0.2
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```


Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to pass in the parameter "binomial" which means we are using the independent Bernoulli. There are other types of models we won't get a chance to study e.g. Poisson, negative binomial.

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = "binomial")
```

Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malginant. If you can read log odds, you'll see ... has a small change of being malignant and ... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

It's very sure of itself! 

Let's see $y$ by $\hat{p}$ another way:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there...

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Still tends to be so sure of itself.

```{r}
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Not bad... and the Brier score?

```{r}
mean(-(y_test - p_hats_test)^2)
```

Not as good but still very good!

```{r}
rm(list = ls())
```

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than $50K per year and 0 if they make less than $50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 2,000 and 5,000 to run experiments:

```{r}
train_size = 2000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model:

```{r}
logistic_mod = glm(income ~ ., adult, family = "binomial")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

Much more humble!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and usually confident about the small incomes.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
table(as.numeric(y_train)) #casting works... almost...
y_train_numeric = as.numeric(y_train) - 1
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is a decent Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Looks similar to training.

```{r}
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Not bad... and the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyon understands classification errors!

# Using Probability Estimation to do Classification

Let's establish a rule: if the probability estimate is greater than or equal to 50%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.5, ">50K", "<=50K"))
```

How did this "classifier" do in-sample?

```{r}
mean(y_hats_train != y_train)
table(y_train, y_hats_train)
```

Let's see the same thing oos:

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.5, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

A tad bit worse. Here are estimates of the future performance for each class:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

This whole classifier hinged on the decision of 50%! What if we change it??

# Asymmetric Cost Classifiers

Let's establish a *new* rule: if the probability estimate is greater than or equal to 90%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.9, ">50K", "<=50K"))
mean(y_hats_train != y_train)
oos_conf_table = table(y_train, y_hats_train)
oos_conf_table
```

Of course the misclassification error went up! But now look at the confusion table! The second column represents all $\hat{y} = 1$ and there's not too many of them! Why? You've made it *much* harder to classify something as positive. Here's the new additional performance metrics now:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

We don't make many false discoveries but we make a lot of false omissions! It's a tradeoff...

# Receiver-Operator Curve Plot

The entire classifier is indexed by that indicator function probability threshold which creates the classification decision. Why not see look at the entire range of possible classification models. Here's all the thresholds we'll consider numerically:

```{r}
RES = 0.01
p_thresholds = seq(0 + RES, 1 - RES, by = RES) #values of 0 or 1 are trivial
```

Now let's create a table that houses all sorts of metrics:

```{r}
performance_metrics_in_sample = matrix(NA, nrow = length(p_thresholds), ncol = 12)
colnames(performance_metrics_in_sample) = c(
  "p_th",
  "TN",
  "FP",
  "FN",
  "TP",
  "miscl_err",
  "precision",
  "recall",
  "FDR",
  "FPR",
  "FOR",
  "miss_rate"
)
```

Each row will represent a model and record the performance of that model so we can (1) pick a "best" model at the end and (2) overall understand the performance of the probability estimates a la the Brier scores, etc.

```{r}
n = length(y_train)
for (i in 1 : length(p_thresholds)){
  p_th = p_thresholds[i]
  y_hats_train = factor(ifelse(p_hats_train >= p_th, ">50K", "<=50K"))
  in_sample_conf_table = table(
    factor(y_train, levels = c("<=50K", ">50K")),
    factor(y_hats_train, levels = c("<=50K", ">50K"))
  )
    
  fp = in_sample_conf_table[1, 2]
  fn = in_sample_conf_table[2, 1]
  tp = in_sample_conf_table[2, 2]
  tn = in_sample_conf_table[1, 1]
  npp = sum(in_sample_conf_table[, 2])
  npn = sum(in_sample_conf_table[, 1])
  np = sum(in_sample_conf_table[2, ])
  nn = sum(in_sample_conf_table[1, ])

  performance_metrics_in_sample[i, ] = c(
    p_th,
    tn,
    fp,
    fn,
    tp,
    (fp + fn) / n,
    tp / npp, #precision
    tp / np,  #recall
    fp / npp, #false discovery rate (FDR)
    fp / np,  #false positive rate (FPR)
    fn / npn, #false omission rate (FOR)
    fn / np   #miss rate
  )
}

round(head(performance_metrics_in_sample), 3)
round(tail(performance_metrics_in_sample), 3)
```

Now let's plot the ROC curve

```{r}
ggplot(data.frame(performance_metrics_in_sample)) +
  geom_line(aes(x = FPR, y = recall)) +
  geom_abline(intercept = 0, slope = 1, col = "red")
```







